# 3. Storage and Retrieval

On fundamental level database has to store data somewhere when we put it there and give data back when we ask for it.

It's important to select appropriate storage engine which fits application workload the best.

There's big difference between storage engines for transactional workloads and for analytics (for example).

There're two families of storage engines:
- log structured;
- page oriented.

## Data structures that power your database

[[World's simplest database]]

[[Database Index|Database Index]]

### Hash Indexes

[[Hash Index]]

### SSTables and LSM-Trees

[[SSTable|SSTables]]

#### Making an LSM-tree of [[SSTable|SSTables]]

[[LSM-tree]]

### BTrees

[[B-Tree|B-Tree]]

### Comparing B-Trees and LSM-trees

[[B-Trees vs LSM-Trees]]

### Other indexing structures

[[Secondary Index]]

#### Storing values within the index

Key in index is the thing query searches for, and value is either the row in question or reference to a row. In second case, place where rows are stored is called *heap file*. This approach makes all indices point to the same data (no duplicate).

When stored row is changed, update is quite efficient if new value is not larger than the old one - the old data can be overwritten just in place. If the new value has greater size, then either all indexes have to be updated, or forwarding pointer left instead of old value, and actual value is written elsewhere.

![[Clustered Index#^5263c5]]

Indexes may include some additional columns data. If query requests only those columns, index may _cover the query_, meaning there will be no hop to the heap file. Though, it has trade-off on write, because additional data must be kept up-to-date + transactional enforcements.

#### Multi-column indexes

[[Composite Index|Multicolumn Index]]

For spatial data, R-Tree indexes may be used (PostGIS uses them). Not so commonly, for 2D coordinates range filter (all points within rectangle), the longitude + latitude may be translated into single value using space filling curve.

> Multi-dimensional indexes may be used not only for geographic data. They may be used for any kind of multi-dimensional range queries - like colors range (red, green, blue), date + temperature range and so on.

#### Fuzzy and full-text search indexes

Full text search allows use of synonyms in order to ignore grammatical variations. Also, typos tolerance within given edit distance is allowed.

> Edit distance - number of single character changes (add, remove, replace) for original word to make up final word.

*Lucene* uses similar to [[SSTable|SSTables]] structure in order to store terms dictionary. But in-memory index is somewhat different, because it doesn't have the keys itself, but rather finite state automation (kind of _trie_), which can be transformed into _Levenshtein automation_, allowing to search words within given edit distance.

Other fuzzy search go into direction of document classification and ML.

#### Keeping everything in-memory

Disks are durable, while RAM is volatile. Also, cost per gb for disk is cheaper compared to RAM. Keeping big data sets completely in memory doesn't seem to be feasible for now, but for small ones it can be an option.

Some in-memory databases may be used for cache purposes only (such as Memcached). Others achieve durability by replicating state to other matchines, doing periodic snapshots, writing changelogs to disks or by special hardware (battery-powered RAM).

On db restart it is necessary to load database completely either from disk or from replica over network.

> [[VoltDB]], MemSQL, [[Oracle]] Ten Times offer relational model databases in-memory.

> [[Redis]] and [[CouchBase]] provide weak durability, because write asyncronously to disk.

In-memory databases not always faster because it is not necessary to read from disk. Instead, main gain is because no need to encode data to format which can be stored on disk.

Special data models can easily be implemented when using in-memory storage engine. [[Redis]] provides priority queues and sets.

In-memory databases may store larger data sets than available memory. **Anti-caching** approach implies that least used data may be backed up on disk until it is necessary again. Yet, the indexes are still required to fit entirely in memory.

## Transaction Processing or Analytics

[[Transaction]]

_OLTP_ (online transaction processing) - access pattern, where typically small number of records are selected from DB, some records are inserted and updated based on user's input.

_OLAP_ (online analytic processing) - access patter, which, compared to _OLTP_, doesn't return raw data to user, but rather does run kind of analytic query to scan typically big number of records in order to calculate some stats.

_Data warehouse_ - OLAP-dedicated database separate from main OLTP database.

### Data warehousing

_Data warehouse_ - separate from main read-only database, which contains the transformed copy of OLTP DB data which has analysis-friendly form.

*ETL (extract-transform-load)* - process of getting data to *Data Warehouse*.

The main benefit of having separate OLAP database is that it can be optimized for analytic access paterns (processing of large amount of data).


#### Divergence between OTLP databases and data warehouses

Usually both OLTP and OLAP dbs use SQL interface. But underhood storage strategies are completely different.

Usually database vendors focus either on transactional processing or analytics.

Teradata, Vertica, Sap Hana, PerAccell are really expensive. Amazon Redshift is a hosted version of PerAccell.

There are also open-source solutions based on Apache Hadoop: Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, Apache Drill.

### Stars and Snowflakes: Schemas for Analytics

There are not a lot of schemas for Analytics. 
Usually it is **Star Schema** (AKA *dimensional modeling*).

The *Fact Table* lies in the center of data warehouse schema. This is the table, which represent events occured at some particular time. It has some own attributes (like net price, sell price, etc) and foreign keys to *dimension tables*.

*Dimension Table* - table, which stores data about *who*, *what*, *where*, *when*, *how*, and *why* data about event. 

> Even dates are put to dimension table, allowing to store additional information (like public holiday).


The **Snowflake Schema** is a variation of *Star Schema*, in which dimension tables may be broken down into subdimension tables. This makes schema more normalized. However, *star schema* is more prefered, becuase it is easier for analytics to work with.

> Typically data warehouse tables have a lot of columns (hundreds of), because they store all metadata which may be relevant for analytics.


## Column-Oriented Storage

Having trillions of rows in fact table and millions in dimension tables is real issue.

OLTP (and document) databases  store data in row-oriented fashion - the values which belong to single row are stored together. But in most cases only few columns of hundreds are needed to query. 

When running the analytics query, we can add indexes on column used in joins, however it will be still necessary for DB engine to load all of those rows from disk into memory, and filter out not matching conditions. It can take a lot of time when there are hundreds of columns.

**Column Oriented Storage** - storing together column data instead of row data. It has condition that each column file must contain rows in the same order. To reconstruct the whole 23-rd row, we'd take 23-rd entry from each column file.

### Column Compression

Usually columns are good for compression. Depending on data stored in column, different techniques may be used. 

#### Bitmap encoding

Trillions of records may have only millions of distinct values (for example, number of products which were ever sold is much less than number of all products purchases). 

For every single *unique value*, we can *build a bitmap* representing in which rows this particular *value is being used*. Hence, every bitmap will encode information for all the rows about particular value.

Because there are scads of rows, bitmaps may need to use run-length encoding. Meaning, instead of full ones and zeros chain, each sequence of zeros and ones is represented with single value. For instance, 00011001001111 will be represented as (3, 2, 2, 1, 2, 4).

Bitmap Indexes are very effective for any queries, which may be converted to binary representation. ( `IN`, `AND`, `OR` queries). The `attr IN (32, 41, 84)` will load three bitmaps, do bitwise `OR` operation on them. The `attr1 = 43 AND attr2 = 54` will load bitmap for `attr1` and bitmap for `attr2`, and run bitwise `AND` operation on them. Even though columns are different they both store values for corresponding rows, hence bitwise operations may be safely executed.

> Column families used in [[HBase]] and [[Cassandra]] - columns from single family are stored together. But this is still row-oriented approach (even row key is added to each such record), and column compression is not used.

#### Memory bandwidth and vectorized processing

For queries which scan over millions of rows, a big bottleneck is memory bandwidth. Column-oriented storage reduces the number of data necessary to load into memory, hereby reducing usage of it.

Also, column-oriented storage allows CPU cycles optimization, because processor can load batch of compressed data into L1 cache and work with it in tight loop without function calls.

*Vectorized processing* - bitwise `AND` and `OR` operators may be designed to work directly on chunks of compressed column data. 

### Sort order in Column Storage

Column files may store data in order it was written, meaning when new row comes in, each column file is appended with a corresponding value from incoming row.

However enforcing some order may be helpful as an indexing mechanism. For example, if most queries have date condition, it makes sense to sort all columns data by date column.

> Sorting every column doesn't make any sense, because n-th value from any column must correspond to the same n-th value from any other column in order to make up a row.

We may sort table only by chain of columns, and that's it. 

The sorting is good for compression, especially if first column has low [[Cardinality]]. If sorted data is run-length encoded, billions of rows may be compressed just to few kilobytes.

Other columns in chain are compressed worse. The worst for compression probably is last column, as most likely it will have random data.

#### Several different sort orders

As the data is anyway replicated on multiple machines, we can sort it in several different ways, each of which best suits for some queries. This idea was implemented in C-Store in Vertica data warehouse.

> Big difference to secondary indexes from row-oriented world is that in column store, it will be exact copy of data sorted differently, while secondary indexes just point to data in heap file.


### Writing to Column-Oriented Storage

Column-oriented approach, sorting, compression help a lot for read requests, however it makes a write queries more difficult.

> It is not possible to update in place (like with BTree), because of compression. Consider run-length encoding of bitmask of column data - if it had 45 zeroes, and on update value 19 is gonna change to one. Then updated part should look like 18, 1, 26 instead of just 45. Also, imagine value 45 changed to one - then next encoded number should also change.

New row insertion has to write to all column files. Updating single row makes all column files to be rewritten.

In order to tackle with writes *LSM-trees* are used. All writes at first go to memory-tree until some limit, upon reaching which the data is merged with column files and written to new files. 

> Vertica works essentially this way.

This makes read requests a bit more difficult, because queries have to check both in-memory tree and column files. Though, it is hidden under the hood. For user it doesn't make any difference.

### Aggregation: Data Cubes and Materialized Views

**Materialized Aggregates** - cached data for aggregate queries.

**Virtual View** - table-like object, which acts like a shortcut for the underlying query.
**Materialized View** - table-like object, which stores cached results of an underlying query.

When underlying data changes, the *Materialized View* should be refreshed. But if refresh it on every write, it will slow down write requests.

**Data Cube** (*OLAP Cube*) - special case of *Materialized View*. Basically it is aggregates grid grouped by different dimensions. Each cell contains aggregate (e.g. SUM) of given attribuet (e.g. net_price) for given dimensions (e.g. products, dates) in facts table (e.g. sales).

Data Cubes allow not only get aggregates by combination of dimensions, but also find aggregation by any dimension.

> Advantage of Data Cubes is that some queries become really fast. To get aggreate of yesterday's sales - no longer need to scan over millions of records.

> Disadvantage of Data Cubes is that we have no such flexibility as when having raw data. Finding proportion of items having price greater than 100$ by yesterday is not possible using data cube.


## Summary

Storage engines fall into categories:

- OLTP - designed to quickly hande a lot of small queries usually by some key. Disk seek time is bottleneck here. Typically row-oriented storage is used here.
- OLAP - designed to quickly handle small number of complicated queries, which require scan over lots of data. Disk bandwidth is bottleneck here. Typically column-oriented storage is used here.

OLTP:
- Log-structured approach. Writes usually come down to appending data to a file. Bitcask, [[SSTable|SSTables]], LSM-trees, [[LevelDB]], [[Cassandra]], [[HBase]], Lucene.
- Update-in-place approach. Writes usually update data where it was previously written or in completely new place if there's not enough size. BTrees.

OLAP:

Indexes become much less relevant here, because anyway queries have to scan over millions of records. The core issue is to store data compactly. Column-oriented storage and Compression helps to achieve this goal.












