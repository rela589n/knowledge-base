# 3. Storage and Retrieval

On fundamental level database has to store data somewhere when we put it there and give data back when we ask for it.

It's important to select appropriate storage engine which fits application workload the best.

There's big difference between storage engines for transactional workloads and for analytics (for example).

There're two families of storage engines:
- log structured;
- page oriented.

## Data structures that power your database

[[World's simplest database]]

The idea behind _indexes_  is to keep some meta information on the side, which is like signpost (pointer) to data we want.

Maintaining _additional_ data structures incurs write overhead. When data is inserted, all the indexes have to be updated. It is important trade-off between write and read performance.

### Hash Indexes

> **Hash index** is just **hash map**, which stores **position of data on the disk**. The [[Riak]]'s default storage engine for _BitCask_ has great performance for both reads and writes. Writes are done to _log file_, and for reads there's an in-memory hash-map containing positions for all the data.

Such approach is well suited for intensive writes. Reads are subject to the requirement that **all key hashes should fit into RAM**.

As values are always appended to database, the **disk space issue** may arise. Solution for running out of disk space problem:

**Break the log into segment files** such that when current segment overlaps some limit, new segment file is started. Hence, **committed segments may be compacted** without locking write operations. 

> **Compaction** - removing duplicated keys and saving only latest value for each particular key.

Moreover, as a lot of keys may be thrown away during segment compaction, it is possible to **merge multiple segments into single**, doing compaction of values at the same time.

**Existing segments are never modified**, hence **merging is done in background**, and it doesn't lock neither read nor write operations. When merge finished, final segment will replace ones it was built upon.

Each segment maintain it's own in-memory hash-map. Not a lot fo segments should exist in order for lookup to be quick.

Real implementation has to handle those issues:
- **File format**. The text files like CSV or similar have overhead for escaping. The best fit here is **binary log file**. Format is such: length of data in bytes followed by this data;
- **Deletion**. In order to delete records, **tombstone record** is created;
- **Crash Recovery**. Reloading all the hash maps into memory on server restart may take a lot of time. Some implementations **do snapshots of each map** on disk;
- **Partially written records**. Writes log file contains each record **checksum** in order to detect and ignore corrupted parts;
- **Concurrency control**. As log is sequential append-only, **writes** are done **in single thread**. While **reads may be concurrent**;

Such design has pros:
- **append-only** is usually **faster than random write**;
- **append-only** makes **crash recovery easier** - no need to handle case when only part of old data was overwritten;
- **merging old segments prevents** problem of **fragmented files** over time.

And cones:
- hash **table must fit into memory**;
- **range queries not supported** (i.e. all keys from kitty00000 to kitty99999).

### SSTables and LSM-Trees

[[LevelDB]], [[RocksDB]]

**SSTable** stands for **Sorted String Table**. 
Data is stored likewise in segment files, however within segment all **records are sorted by keys**, and there is **no key duplicates** per segment file.

Advantages:
1. **Performant segments merge** (with cost of `O(n+m)`).
2. **Not all indexed keys are kept in the hashmap**. The **sparse index** map is kept anyway in memory (one **key per every few kilobytes**). To find a value, adjacent keys present in index are checked, and file is scanned from left pointer until reaching right pointer or until value is found.
3. **Values** may be **grouped in blocks** and **compressed** before writing on the disk since during the read **we scan the file anyway**. Thus every such scan range will be in compressed state in file.

#### Constructing and Maintaining SSTables

Tree data structures allow us to insert values in any order and get them back in sorted order. Insertion is done usually with `O(log(n))` . AVL, Red-Black trees may be used, becuase they imply self-balancing on modification.

To **maintain SSTables** storage engine, there would be necessary:
- when writes comes in, **new value** is **appended to the in-memory tree** (memtable).
- once **memtable is big enough** (few MB), **it's flushed** to the disk. The tree will be written effectively as SSTable, because it already maintains order of keys. Every **flush will create a new segment**. During the flush, **new writes** are written **to a new memtable**.
- **to serve the read** requests, first **check the in-memory tree** for given key. If not found, then one by one **check recent segments**.
- **in background** it's necessary to **merge segments** from time to time to discard deleted or overwritten records.
- **for failure-resilience**, when write comes in we can also **write value to the log file**, which will be cleared once SSTable written on disk. This way, **no inserts are lost** when server crashes before memtable gets written to the disk.

> This algo is used in [[LevelDB]], [[RocksDB]]. Similar storage engines are used in [[Cassandra]] and [[HBase]]

#### Making an LSM-tree of SSTables

_LSM_ stands for _Log-Structured Merge-Tree_.

LSM storage engines are based on this principle of merging and compacting sorted files.

Lucene (indexing engine used by ElasticSearch and Solr) uses similar method for storing its _term dictionary_. The mappings from terms to postings list is kept in SSTable-like files, which are merged when needed.

##### Performance Optimizations

LSM-tree algorithm may be slow when looking for keys, which are not present in database.  It requires to check all segments in their order. This kind of acces is optimized by _Bloom filters_ - data structure for approximating the contents of a set. It can tell if key doesn't exist using bitmasks.

Another topics are strategies how to compact and merge SSTables. Most common are:
- size-tiered ([[HBase]], [[Cassandra]]);
- leveled ([[LevelDB]], [[RocksDB]], [[Cassandra]]).

Size-tiered strategy successively merges newest (and smallest) SSTables into oldest (and largest) ones.
Leveled strategy requires less disk space and makes more incremental compaction.

> LSM-trees allow range queries (scan for keys from L unto R)

### BTrees

[[BTree|BTree]]

### Comparing B-trees and LSM-trees

Generally it is considered that B-trees are faster for read than LSM-trees, while LSM-trees are faster for write. Also, B-Trees are more mature and widely used.

#### Advantages of LSM-trees

B-tree storage engine must write data at least twice: to [[Write-Ahead Log|WAL]] and to the index itself. And probably once more when page split is required (on overflow).

Log-structured indices have to do some additional job (compaction and merging of segments). When one incoming write to DB produces multiple writes to disk over DB lifetime is called _write amplification_. 

LSM-trees typically have higher write throughput than B-trees partly because sometimes write amplification is lower, and partly because they are not page-oriented, and don't have to overwrite the whole page on disk when few bytes were updated, but instead write compact SSTables sequentially.

LSM-trees are compressed better. No fragmentation is possible with LSM-trees, hence it takes less storage overhead (especially with leveled compaction).

On SSD, lover _write amplification_ and absense of fragmentation is beneficial: it allows more writes and reads within available I/O bandwidth.

#### Downsides of LSM-trees

The background merge of segments may interfere with incoming reads/writes. Even though, conceptually two are independent, they still use same shared resource - disk. Disks have limited resources, which means that some request will need to __wait until__ disk __finishes__ expensive __compaction__ operation. The impact is visible at hight percentiles.

When write throughput is high, the background compaction may not keep up with incoming writes. This slows down both write and read throughput, because the incoming writes share disk write bandwith with background process, and reads have to check a lot of segments to find value. The higher database gets, the bigger disk bandwidth is required for compaction.

The advantage of B-tree is that any particular key may exist only in single place of database. It offers ease of implementation of transactional processing, which is usually implemented using locks on keys ranges. Locks may be attached directly to tree.

> It is better to test workload empirically on different storage engines to choose the best.

### Other indexing structures

[[Secondary Index]]

#### Storing values within the index

Key in index is the thing query searches for, and value is either the row in question or reference to a row. In second case, place where rows are stored is called *heap file*. This approach makes all indices point to the same data (no duplicate).

When stored row is changed, update is quite efficient if new value is not larger than the old one - the old data can be overwritten just in place. If the new value has greater size, then either all indexes have to be updated, or forwarding pointer left instead of old value, and actual value is written elsewhere.

![[Clustered Index#^5263c5]]

Indexes may include some additional columns data. If query requests only those columns, index may _cover the query_, meaning there will be no hop to the heap file. Though, it has trade-off on write, because additional data must be kept up-to-date + transactional enforcements.

#### Multi-column indexes

The most common type is **concatenated index**. Columns are concatenated to make a single key. Index, consisting of (firstname+lastname) may be used to find records either by firstname or by both firstname and lastname.

For spatial data, the R-Tree indexes may be used (PostGIS uses them). Not so commonly, for 2D coordinates range filter (all points within rectangle), the longitude + latitude may be translated into single value using space filling curve.

> Multi-dimensional indexes may be used not only for geographic data. It may be used for  any kind of multi-dimensional range queries - like colors range (red, green, blue), date + temperature range and so on.


#### Fuzzy and full-text search indexes

Full text search allows use of synonyms in order to ignore grammatical variations. Also, typos tolerance within given edit distance is allowed.

> Edit distance - number of single character changes (add, remove, replace) for original word to make up final word.

*Lucene* uses similar to SSTables structure in order to store terms dictionary. But in-memory index is somewhat different, because it doesn't have the keys itself, but rather finite state automation (kind of _trie_), which can be transformed into _Levenshtein automation_, allowing to search words within given edit distance.

Other fuzzy search go into direction of document classification and ML.

#### Keeping everything in-memory

Disks are durable, while RAM is volatile. Also, cost per gb for disk is cheaper compared to RAM. Keeping big data sets completely in memory doesn't seem to be feasible for now, but for small ones it can be an option.

Some in-memory databases may be used for cache purposes only (such as Memcached). Others achieve durability by replicating state to other matchines, doing periodic snapshots, writing changelogs to disks or by special hardware (battery-powered RAM).

On db restart it is necessary to load database completely either from disk or from replica over network.

> [[VoltDB]], MemSQL, Oracle Ten Times offer relational model databases in-memory.

> [[Redis]] and [[CouchBase]] provide weak durability, because write asyncronously to disk.

In-memory databases not always faster because it is not necessary to read from disk. Instead, main gain is because no need to encode data to format which can be stored on disk.

Special data models may be easily implemented when storage engine is in-memory. [[Redis]] provides priority queues and sets.

In-memory databases may store larger data sets than available memory. **Anti-caching** approach implies that least used data may be backed up on disk until it is necessary again. Yet, the indexes are still required to fit entirely in memory.

## Transaction Processing or Analytics

[[Transition]]

_OLTP_ (online transaction processing) - access pattern, where typically small number of records are selected from DB, some records are inserted and updated based on user's input.

_OLAP_ (online analytic processing) - access patter, which, compared to _OLTP_, doesn't return raw data to user, but rather does run kind of analytic query to scan typically big number of records in order to calculate some stats.

_Data warehouse_ - OLAP-dedicated database separate from main OLTP database.

### Data warehousing

_Data warehouse_ - separate from read-only database, which contains the transformed copy of OLTP DB data which has analysis-friendly form.

*ETL (extract-transform-load)* - process of getting data to *Data Warehouse*.

The main benefit of having separate OLAP database is that it can be optimized for analytic access paterns (processing of large amount of data).


#### Divergence between OTLP databases and data warehouses

Usually both OLTP and OLAP dbs use SQL interface. But underhood storage strategies are completely different.

Usually database vendors focus either on transactional processing or analytics.

Teradata, Vertica, Sap Hana, PerAccell are really expensive. Amazon Redshift is a hosted version of PerAccell.

There are also open-source solutions based on Apache Hadoop: Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, Apache Drill.

### Stars and Snowflakes: Schemas for Analytics

There are not a lot of schemas for Analytics. 
Usually it is **Star Schema** (AKA *dimensional modeling*).

The *Fact Table* lies in the center of data warehouse schema. This is the table, which represent events occured at some particular time. It has some own attributes (like net price, sell price, etc) and foreign keys to *dimension tables*.

*Dimension Table* - table, which stores data about *who*, *what*, *where*, *when*, *how*, and *why* data about event. 

> Even dates are put to dimension table, allowing to store additional information (like public holiday).


The **Snowflake Schema** is a variation of *Star Schema*, in which dimension tables may be broken down into subdimension tables. This makes schema more normalized. However, *star schema* is more prefered, becuase it is easier for analytics to work with.

> Typically data warehouse tables have a lot of columns (hundreds of), because they store all metadata which may be relevant for analytics.


## Column-Oriented Storage

Having trillions of rows in fact table and millions in dimension tables is real issue.

OLTP (and document) databases  store data in row-oriented fashion - the values which belong to single row are stored together. But in most cases only few columns of hundreds are needed to query. 

When running the analytics query, we can add indexes on column used in joins, however it will be still necessary for DB engine to load all of those rows from disk into memory, and filter out not matching conditions. It can take a lot of time when there are hundreds of columns.

**Column Oriented Storage** - storing together column data instead of row data. It has condition that each column file must contain rows in the same order. To reconstruct the whole 23-rd row, we'd take 23-rd entry from each column file.

### Column Compression

Usually columns are good for compression. Depending on data stored in column, different techniques may be used. 

#### Bitmap encoding

Trillions of records may have only millions of distinct values (for example, number of products which were ever sold is much less than number of all products purchases). 

For every single *unique value*, we can *build a bitmap* representing in which rows this particular *value is being used*. Hence, every bitmap will encode information for all the rows about particular value.

Because there are scads of rows, bitmaps may need to use run-length encoding. Meaning, instead of full ones and zeros chain, each sequence of zeros and ones is represented with single value. For instance, 00011001001111 will be represented as (3, 2, 2, 1, 2, 4).

Bitmap Indexes are very effective for any queries, which may be converted to binary representation. ( `IN`, `AND`, `OR` queries). The `attr IN (32, 41, 84)` will load three bitmaps, do bitwise `OR` operation on them. The `attr1 = 43 AND attr2 = 54` will load bitmap for `attr1` and bitmap for `attr2`, and run bitwise `AND` operation on them. Even though columns are different they both store values for corresponding rows, hence bitwise operations may be safely executed.

> Column families used in [[HBase]] and [[Cassandra]] - columns from single family are stored together. But this is still row-oriented approach (even row key is added to each such record), and column compression is not used.

#### Memory bandwidth and vectorized processing

For queries which scan over millions of rows, a big bottleneck is memory bandwidth. Column-oriented storage reduces the number of data necessary to load into memory, hereby reducing usage of it.

Also, column-oriented storage allows CPU cycles optimization, because processor can load batch of compressed data into L1 cache and work with it in tight loop without function calls.

*Vectorized processing* - bitwise `AND` and `OR` operators may be designed to work directly on chunks of compressed column data. 

### Sort order in Column Storage

Column files may store data in order it was written, meaning when new row comes in, each column file is appended with a corresponding value from incoming row.

However enforcing some order may be helpful as an indexing mechanism. For example, if most queries have date condition, it makes sense to sort all columns data by date column.

> Sorting every column doesn't make any sense, because n-th value from any column must correspond to the same n-th value from any other column in order to make up a row.

We may sort table only by chain of columns, and that's it. 

The sorting is good for compression, especially if first column has low [[Cardinality]]. If sorted data is run-length encoded, billions of rows may be compressed just to few kilobytes.

Other columns in chain are compressed worse. The worst for compression probably is last column, as most likely it will have random data.

#### Several different sort orders

As the data is anyway replicated on multiple machines, we can sort it in several different ways, each of which best suits for some queries. This idea was implemented in C-Store in Vertica data warehouse.

> Big difference to secondary indexes from row-oriented world is that in column store, it will be exact copy of data sorted differently, while secondary indexes just point to data in heap file.


### Writing to Column-Oriented Storage

Column-oriented approach, sorting, compression help a lot for read requests, however it makes a write queries more difficult.

> It is not possible to update in place (like with BTree), because of compression. Consider run-length encoding of bitmask of column data - if it had 45 zeroes, and on update value 19 is gonna change to one. Then updated part should look like 18, 1, 26 instead of just 45. Also, imagine value 45 changed to one - then next encoded number should also change.

New row insertion has to write to all column files. Updating single row makes all column files to be rewritten.

In order to tackle with writes *LSM-trees* are used. All writes at first go to memory-tree until some limit, upon reaching which the data is merged with column files and written to new files. 

> Vertica works essentially this way.

This makes read requests a bit more difficult, because queries have to check both in-memory tree and column files. Though, it is hidden under the hood. For user it doesn't make any difference.

### Aggregation: Data Cubes and Materialized Views

**Materialized Aggregates** - cached data for aggregate queries.

**Virtual View** - table-like object, which acts like a shortcut for the underlying query.
**Materialized View** - table-like object, which stores cached results of an underlying query.

When underlying data changes, the *Materialized View* should be refreshed. But if refresh it on every write, it will slow down write requests.

**Data Cube** (*OLAP Cube*) - special case of *Materialized View*. Basically it is aggregates grid grouped by different dimensions. Each cell contains aggregate (e.g. SUM) of given attribuet (e.g. net_price) for given dimensions (e.g. products, dates) in facts table (e.g. sales).

Data Cubes allow not only get aggregates by combination of dimensions, but also find aggregation by any dimension.

> Advantage of Data Cubes is that some queries become really fast. To get aggreate of yesterday's sales - no longer need to scan over millions of records.

> Disadvantage of Data Cubes is that we have no such flexibility as when having raw data. Finding proportion of items having price greater than 100$ by yesterday is not possible using data cube.


## Summary

Storage engines fall into categories:

- OLTP - designed to quickly hande a lot of small queries usually by some key. Disk seek time is bottleneck here. Typically row-oriented storage is used here.
- OLAP - designed to quickly handle small number of complicated queries, which require scan over lots of data. Disk bandwidth is bottleneck here. Typically column-oriented storage is used here.

OLTP:
- Log-structured approach. Writes usually come down to appending data to a file. Bitcask, SSTables, LSM-trees, [[LevelDB]], [[Cassandra]], [[HBase]], Lucene.
- Update-in-place approach. Writes usually update data where it was previously written or in completely new place if there's not enough size. BTrees.

OLAP:

Indexes become much less relevant here, because anyway queries have to scan over millions of records. The core issue is to store data compactly. Column-oriented storage and Compression helps to achieve this goal.












