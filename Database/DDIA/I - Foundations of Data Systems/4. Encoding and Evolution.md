# 4. Encoding and Evolution

Changing application requirements may require changes to data it stores.

When format of data or it's schema changes, the code which works with this data generally need to be changed. For instance, when adding new field to record, the code is gonna work with it somehow.

In applications code change is not done instantly:

- With *server-side* applications it is possible to perform *rolling upgrade* (*staged rollout*). Deploy code to few nodes, check if works fine, - gradually deploy to rest. This allows zero service downtime, thus more frequent releases, - better _evolvability_.
- With *client-side* applications, it is up to user whether or not to install updates.

Old and new versions of code, old and new data formats may coexist in system at the same time. This makes us to maintain compatibility in both directions:

- [[Backward compatibility]];
- [[Forward compatibility]]

## Formats for Encoding Data

There are two representations of data:
- in-memory - commonly in data structures (objects, arrays) and pointers;
- encoded - self-contained sequence of bytes for representation or storage.

Hence, it is necessary to have transition layer between two.

Translation in-memory -> encoded is called **encoding** (aka serialization, marshalling).
Translation encoded -> in-memory is called **decoding** (aka deserialization, unmarshalling).

### Language-specific formats

There are `Java.IO.Serializable` for Java, `Marshal` for Ruby, `pickle` for Python built-in traits for encoding. They are convenient, because make it easy to encode and decode objects with minimal amount of additional code.

There are deep problems:
- encoding is often _tied to programming language_. Saving and transmitting data in this format precludes usage of other languages;
- decoding data requires _instantiation of arbitrary classes_. If attacker can modify   byte sequence used to decode, he'll be able to instantiate any class, and potentially execute any code remotely;
- _versioning_ is afterthought - such libraries often neglect the problems of BC and FC.
- _efficiency_ is afterthought - Java's serialization is notorious for bad performance.

### JSON, XML, Binary formats

Json and XML, CSV are contenders for standartized encodings.

Though, they have flaws:
- ambiguity of numbers and numeric strings. CSV and XML doesn't distinguish them at all (except by referring to external schema). JSON distinguishes numbers and strings, but it doesn't discern integers and floats, and doesn't specify precision. Numbers greater than 2^53 can not be represented with `double` type and may become inaccurate during parsing in language which uses `double` (hello, JavaScript). The Twitter includes two IDs: one as number, one as string to tackle this problem;
- JSON and XML have support of Unicode, but doesn't have support for binary strings. Common workaround is to `base64encode` binary strings. Though, it increases size by around 33%. One more thing - we need schema to indicate that value is represented as base64;
- Optional schemas exist for XML and JSON. XML widely uses it, but many JSON-based tools doesn't support it, hence it makes us anyway to hard-code encoding/decoding logic;
- CSV is quite vague format. If data need to have comma or new line in it, it has to be respectively escaped, but even though, not all parsers handle it correctly.

The JSON, XML, and CSV are likely to remain popular, especially as data interchange formats. As far as two organizations agreed on format of data - rest doesn't matter.

#### Binary encoding

### Thrift and Protocol Buffers

[[Binary data formats]]

#### Field Tags and Schema Evolution

It is possible to __change field names__ without BC breaks, because encoded data is not aware of field names (it uses field tags). However it is not possible to change [[Field tag|field tag]], because all existing documents would become invalid.

If we **add a new field type**, in order to maintain forward compatibility, old code should ignore it. That's why type is encoded along with data - we have to know how much bytes to skip if encounter unknown type. In order to maintain backward compatibility, no required fields are allowed after initial deployment.

If we are about to **remove field**, it is possible to remove only optional fields. Required fields can never be removed. One more thing is that this tag number may never be used again, because some old data with this tag may encounter new code.

#### DataTypes and schema evolution

Extending int32 to int64 is allowed (parser can handle it), however vice versa will make values truncated.

With *Protocol Buffers* it is possible to upgrade a *single-valued* `optional` field into `repeated` *multi-valued*. This is _not_ possible with Thrift. However it has advantage of supporting nested lists.

### Avro

This is completely different from [[Thrift]] and [[Protobuf|Protocol Buffers]]. It gives the most compact encoding.

It uses data schema, however encoded data have **no [[Field tag|field tag]] numbers**. It also uses varints.

Encoded data is put "as is" meaning that take schema, and one by one encode each field of this schema into output.

Hence, in order to read data it is necessary to go through schema fields and correlate it with encoded data. To parse actual valuse, type definitions from schema are used.

> Any mismatch between reader's and writer's schema would mean incorrectly decoded data

#### The writer's schema and the reader's schema

**Writer's schema** - schema, used for data encoding.
**Reader's schema** - schema, used for data decoding.

These two don't have to be the same. They just **have to be compatible**. When Avro library reads data, it checks the *difference between schemas*, *translating* (correlating) *writer's schema* into *reader's schema*.

Fields, which are present in R, but not in W are filled with default values from R schema.
Fields, wich are present in W, but not in R are ignored.
Fields, present in both sides are correlated by their names.

#### Schema evolution rules

Writer with old schema, Reader with new schema - backward compatibility;
Writer with new schema, Reader with old schema - forward compatibility.

To maintain compatibility, it is only possible to add/remove fields with default value.

Default value must have type of field, meaning that if we'd like to put null instead of integer, we will have to use `union {null, int}` instead of plain int.

> Avro doesn't have `optional` and `required` markers as Thrift and Protocol Buffers does. It does have `union` and default values instead.

**Adding new type to union** is backward compatible, but not forward.
**Renaming field** is also backward compatible (using field aliases for old write schema), but not forward.

#### What is the writer's schema?

How does reader supposed to know which schema was used to write the data in order to compare itself with it?

It depends on context where used:

- *Large file with lots of records* - for storing millions of records. Writer just includes it's schema at the beginning of the file;
- *Database with individually written records* - each record may have different schema. Schemas versions are stored separately. Each *record points to schema version* used;
- *Sending records over network* - two processes negotiate schema version, then send actual data.

> It is useful to have database of schema versions, because in any time we can check compatibility.


#### Dynamically generated schemas

Avro has advantage of not having tag numbers in it's schema. Having those will complicate auto-generate schema.

As far as Avro does allow to declare schema in JSON document, we may generate such from relational database schema, and dump data to Avro object container file.

When **database schema changes** (imagine, on column addition and one removal), the new dynamically generated **schema may be used right away**. No need to take care of any tag updates. The updated schema still can be matched to old reader's schema.

#### Code generation and dynamically typed languages

Usually schema is useful for static languages, because such have explicit compilation step and efficient in-memory structures may be used for decoded data structures. For dynamic languages it makes less sense.

When it comes to dynamic schema provided by Avro, code generation is unnecessary obstacle to getting to the data.

With Avro object container file it is possible to work the same way as with JSON.

### The Merits of Schemas

Avro, Protocol Buffers, Thrift define schema, which is much more simple than XML or JSON schemas, and support more grained validation rules (like int in rage or string matches regex).

Binary encoding formats:
- more compact than various binary json or so, because field names are omitted in data;
- the schema is always up-to-date documentation, because it is required for 
encoding and decoding;
- saving schemas in database allows compatibility checks before anything is deployed;
- for statically typed languages code-generation from schema tools are available. It enables type checking at compile time;

> The schema evolution allows the _same schemaless flexibility_ as databases with ordinary jsons provide, but it provide better guarantees about data.

## Modes of DataFlow

Whenever we need to send some data from one process to another when they don't share same memory, data has to be encoded.

There are multiple ways to send data
- through databases;
- through service calls (REST, RPC);
- message passing dataflow.

### DataFlow throught databases

One process writes to database some data. Another process reads data from database. Another process may even be the future version of process that wrote data.

**Backward compatibility** has to be maintained in this flow, otherwise new code won't be able to read data written by old code.

During *rolling upgrade*, there may be some instances with older code and some with newer code. Both are working with same database.

**Forward compatibility** is also necessary here, because newer code may have written some values into database, and subsequently another process reads this data.

One more caveat is about **preserving unknown fields**. Consider that new version of app added field to table and code wrote record to database. Then oler version read record, modify somehow and write it back. This *write back* operation should not clean out newly written only because it doesn't know about it. 

#### Different values written at different times

When new version of server-side application is deployed, new code version is rolled out within few minutes. When it comes to data, old datum will still be present in database in original encoding as it was written (unless explicitly rewritten), this is known as **data outlives the code**.

**Data migrations** are possible, but it is not always the case because of high expenses on large datasets (on such migrations are usually avoided). Simple schema changes are allowed (like add column with default null) without rewriting existing data.

> Schema evolutions make entire database appear as if it was encoded with single schema, even though underlying records may be encoded with several historical schema versions.

#### Archival storage

 Data dump typically encode records with latest schema version, since data is copied anyway.

Since data after dump is thereafter immutable, it makes sense to use Avro object container files. Another option is to use column-oriented storage format such as Parquet.

### DataFlow through services (REST, RPC)

**Service oriented architecture** (SOA), aka **microservices** - service application is decomposed such way that server makes requests to smaller services, which are split by functionality, when it needs some data from it. Typically, server is a client to another services.

Services can impose application-specific restrictions on what clients are allowed or not allowed to do, unlike databases which allow arbitrary access. 

Key goal of microservices is to make **independent parts** of application **independently deployable and evolvable**. The same alike way, there may be old and new versions of clients and services that interact.

#### Web services

**Web service** - service, which is allowed to access using HTTP.

Applications:
- mobile or spa applications communicate with web service;
- one service calls another service within same organization (typically known as **middleware**);
- one service calls service owned by another organization. Usually public APIs are provided (OAuth, payment systems, etc).

There are two approaches for web services:
- **REST** - design approach on HTTP;
- **SOAP** - XML-based protocol for making api requests.

**REST** emphasizes simple data formats, using URLs for resources identification, and using HTTP features for authentication, cache control, content-type negotiation.

*Api* designed according to the principles of *REST* is called **RESTful**.

**SOAP** aims to be independent from HTTP. Instead of HTTP features it comes with **web service framework** (*WS-\**).

*SOAP* api is described using **Web Service Description Language**. *WSLD* supports code generation so that it is possible to use classes and method calls and *web service framework* will handle encoding and decoding of objects.

> WSDL is not designed to be human-readable

#### The problems with remote procedure calls (RPCs)

**RPC** provides **location transparency**, meaning method call over network look the same as local method call.

Although it seems convenient at first, **design is fundamentally flawed**, because network request is fundamentally different from local calls:
- local function outcome depends on given arguments, which we have control over. Network requests rely on **network**, which don't control and which **may be unavailable**. Also, remote machine may be slow or unavailable. Retrying failed request should be anticipated;
- local function either return result or throw exception or never return (infinite loop). Network **requests** may return no result because of **timeout**. In this case we have no idea whether request got through or not;
- remote logic may be **executed multiple times** when retrying failed requests (in case if requests were processed, but responses were lost). Remote server has to implemet requests deduplication (*idempotence*);
- calling local function repeatedly usually takes about the same time. Network request is much slower and its **time to execute varies** from multiple milliseconds up to seconds depending on network load;
- calling local function we pass primitives or pointers to objects. Making network requests all the **parameters need to be encoded**;
- client and server may be written in different programming languages, 
which **datatypes may differ** (e.g. numbers greater than 2^53 for JavaScript).

> It doesn't make any sense to make remote call look like a local one. They are just fundamentally different.

#### Current directions of RPC

**Rest.li** uses JSON over HTTP,
**Finagle** uses Thrift,
**gRPC** uses Protocol Buffers.

Today RPC frameworks don't try to make network and local calls unified. On the contrary, *Finagle* and *Rest.li* use **futures** (**promises**) to encapsulate async actions that may fail. *gRPC* uses **streams** - call consists not only from request and response, but from series of requests and responses over time.

> Some RPC frameworks provide *service discovery* - allowing client to find out at which ip and port it can find a particular service.

**The main focus of RPC** frameworks is making **requests** between services owned by the same organization typically **within the same data center**.

Even though binary encoding can achieve better performance, the **RESTful APIs** have significant **advantages**: 
- they are easier to debug (plain HTTP request to endpoint),
- a lot of tooling available (caches, load balancers, firewalls, monitoring, testing tools, etc).

#### Data encoding and evolution for RPC

It is reasonable to assume that all **servers will be upgraded before clients**. In this case we need to maintain **forward compatibility for responses** on client side and **backward compatibility for requests** on server side.

Compatibility properties of RPC scheme are inherited from encoding it uses:
- Thrift, gRPC (Protocol Buffers), Avro RPC can be evolved in accordance with respective encodig format;
- SOAP XML schemas can be evolved, however there are some pitfalls;
- RESTful APIs use JSON. Adding new fields to response, adding optional request parameters are changes that maintain compatibility.

When RPC is used **across organizations**, service compatibility is even harder to maintain, because we **can't force all clients to upgrade**. Hence, compatibility has to be maintained for a long time (perhaps, indefinitely). If breaking change is required then we may end up maintaining multiple versions of service API.

### Message-passing DataFlow

**Asynchronous message-passing** - flow similar to RPC, where client *message* (client request) is delivered to server via *message broker* instead of direct network connection. 

Message broker advantages compared to RPC:
- **higher reliability**, as it can act as buffer if recepients are overloaded;
- **messages are not lost**, because it can automatically redeliver messages to a process that crashed;
- **ip/port are not necessary** to know (we only need to know ip/port of *message broker*);
- **one message to multiple recepients** usually supported by *broker*;
- **sender decoupled from recipient** - it sends messages and doesn't care who will consume them.

Usually this communication is done in one way - sender just publishes messages and forgets about them. 
Though, it is possible for process to send a response, however usuaully this is done through separate channel.

#### Message brokers

Commercial: TIBCO, IBM WebSphere, webMethods.
Open-source: RabbitMQ, ActiveMQ, HornetQ, NATS, Apache Kafka.

One process sends *message* to *queue* or *topic*. Broker delivers this message to one or more *consumers* or *subscribers* of this *queue* or *topic*.

In most cases flow is one-way. However consumer may itself publish messages to another topic or to a reply queue that is consumed by original sender (request/response dataflow similar to RPC).

> Any encoding format may be used to send messages. Brokers accept just a sequence of bytes as a message.

Consumers that **republish** messages to another topics must **preseve unknown fields**, because perhaps it was published by newer version of publisher, while current consumer has not upgraded yet.

#### Distributed actor frameworks

**Actor model** - programming model for achieving concurrency in a single process. Instead of dealing with threads, **logic is encapsulated in actors**. Actors have local state, and do not have access to state of any other actor. **Communication** is done asynchronously **via messages**.

**Distributed actor framework** use this model to scale application across multiple nodes. It essentially integrates *message brokers* and *actor model* into a single framework. Compatibility of messages should be taken into account.

Frameworks:
- **Akka** (Java, Scala) - uses built-in serialization, however it is possible to integrate Protocol Buffers to get an opportunity of rolling upgrades;
- **Orleans** - uses custom encoding format. Rolling upgrades are not possible. To perform upgrade, new node should be provisioned, traffic sent to this node, and then old node shut down;
- **Erlang OTP** - schemas hard to change; rolling upgrades should be planned carefully, experimental *maps* data type may be used for the rescue.

## Summary

During rolling upgrade, the different nodes are running on different state of application. Thus, all data flows in system have to maintain backward (new code read old data) and forward (old code read new data) compatibility.

Encoding formats and compatibility properties:
- language specific - limited to single programming language; fail to provide compatibilities;
- JSON, XML, CSV - widespread, compatibility depends on how to use them. These are vague about data types, especially integers and binary strings. Optional schemas exist, which sometimes are helpful, and sometimes are hindrance;
- Avro, Protocol Buffers, Thrift - binary schema-driven encoding formats, with clearly defined backward and forward compatibility rules.

Data Flows:
- Databases - writing process encods data, reading process decodes it;
- RPC, REST API - client encods request, server decodes it; server encodes response, client decodes it;
- Asynchronous message-passing (using message-brokers or actors) - parties send each other messages; sender encodes it, receiver decodes.
