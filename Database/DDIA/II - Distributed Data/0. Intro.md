# Distributed Data

Reasons to distribute data across multiple machines:
- **Scalability** - when data volume, read/write load grows in such way that one node can no longer handle it, it makes sense to spread the load across multiple nodes;
- **Fault tolerance** / **high availability** - if system is necessary to continue working even if one of machine / network / entire data center goes down, multiple machines may be used in order to achieve redundancy. When one fails, another will take over;
- **Latency** - if system is used worldwide, it makes sense to have multiple servers across the world, so that user requests will be served by the closest server.

## Scaling to higher load

The simplest way is to **scale up** (use **vertical scaling**) - buy more RAM, more CPUs, disks and join them together under one operating system. 

**Shared-memory architecture** - all **joined components** (CPUs, RAM, disks) of the system are **treated as single machine**. It can offer **limited fault tolerance** - hot-swappable components (memory modules, CPU, disks) may be replaced without shutting down.

The main problem here is that **cost increase outweighs the resources increase**. If we scale system twice, cost for it will be definitely increase much more than twice. Also, because of **bottlenecks**, such system won't handle twice as much load.

**Shared-disk architecture** - an array of **disks** is accessible by multiple machines (having independent CPU and RAM) **via fast network** (Network-Attached Storage, Storage Area Network). This architecture may be useful for some data warehouses workloads.

The main problem here is that **scalability is limited** because of connection and locking overhead.

### Shared-nothing architecture

![[Shared-nothing architecture#^b18052]]

![[Shared-nothing architecture#^05e41e]]

### Replication VS Partitioning

The data may be distributed in two ways:

- **Replication** - copy of the **same data** is kept on **multiple nodes**. It provides *redundancy*. If some nodes are unavailable, data can be served from other.
- **Partitioning** (**sharding**) - **data is logically split** across multiple nodes. Each subset is called *partition*.

Often, the *replication* go hand in hand with a *partitioning*.


