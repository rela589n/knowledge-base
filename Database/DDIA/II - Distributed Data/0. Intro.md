# Distributed Data

Reasons to distribute data across multiple machines:
- [[Scalability]];
- [[Availability]]; ^951525
- **Latency** - if system is used worldwide, it makes sense to have multiple servers across the world, so that user requests will be served by the closest server.

## Scaling to higher load

[[Vertical Scaling]]

**Shared-Memory Architecture** - all **joined components** (CPUs, RAM, disks) of the system are **treated as single machine**. It can offer **limited [[Availability|Fault Tolerance]]** - hot-swappable components (memory modules, CPU, disks) may be replaced without shutting down.

The main problem here is that **cost increase outweighs the resources increase**. If we scale system twice, cost for it will be definitely increase much more than twice. Also, because of **bottlenecks**, such system won't handle twice as much load.

**Shared-disk architecture** - an array of **disks** is accessible by multiple machines (having independent CPU and RAM) **via fast network** (Network-Attached Storage, Storage Area Network). This architecture may be useful for some data warehouses workloads.

The main problem here is that **scalability is limited** because of connection and locking overhead.

### Shared-nothing architecture

![[Shared-Nothing Architecture#^b18052]]

![[Shared-Nothing Architecture#^05e41e]]

### Replication VS Partitioning

The data may be distributed in two ways:

- **[[Replication]]** .
- **Partitioning** (**sharding**) - **data is logically split** across multiple nodes. Each subset is called *partition*.

Often, the *replication* go hand in hand with a *partitioning*.


